{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e11e9e",
   "metadata": {},
   "source": [
    "# Kenyan Sign Language Recognition with TensorFlow Lite\n",
    "\n",
    "This notebook demonstrates how to create a TensorFlow Lite model for recognizing Kenyan Sign Language (KSL) and translating it to English text. We'll use the [KSL Dataset from Kaggle](https://www.kaggle.com/datasets/yonaschanie/kslc-dataset) for training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bc8fa",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install and import all the necessary libraries for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow tensorflow-hub matplotlib numpy pandas opencv-python scikit-learn kagglehub pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e51ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0f671",
   "metadata": {},
   "source": [
    "## 2. Download and Explore the Dataset\n",
    "\n",
    "We'll use the Kenyan Sign Language Classification (KSLC) dataset available on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version of the dataset\n",
    "path = kagglehub.dataset_download(\"yonaschanie/kslc-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "!ls -la {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179f424",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset\n",
    "\n",
    "Let's create functions to load and preprocess the images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image dimensions\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8478c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the Kenyan Sign Language dataset\n",
    "    \"\"\"\n",
    "    # This function assumes the dataset is organized in folders by class\n",
    "    # Adjust accordingly based on the actual structure of the dataset\n",
    "    \n",
    "    # Create ImageDataGenerator for data augmentation and normalization\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Load training data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    # Load validation data\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    # Get class names and indices\n",
    "    class_names = sorted(list(train_generator.class_indices.keys()))\n",
    "    print(f\"Found {len(class_names)} classes: {class_names}\")\n",
    "    \n",
    "    return train_generator, validation_generator, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "try:\n",
    "    # First, try to find the dataset structure\n",
    "    !find {path} -type d | sort\n",
    "    \n",
    "    # Assuming the structure contains training data in a directory\n",
    "    # Adjust the path according to the actual dataset structure\n",
    "    dataset_path = path\n",
    "    \n",
    "    # Load the data\n",
    "    train_generator, validation_generator, class_names = load_and_preprocess_data(dataset_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the dataset: {e}\")\n",
    "    print(\"Please adjust the path to point to the folder containing class subfolders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2fa7f",
   "metadata": {},
   "source": [
    "## 3. Create and Train the Model\n",
    "\n",
    "We'll use transfer learning with MobileNetV2, which is both powerful and lightweight - making it ideal for conversion to TensorFlow Lite later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930df491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model based on MobileNetV2\n",
    "    \"\"\"\n",
    "    # Load the MobileNetV2 model with pre-trained weights on ImageNet\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create and train the model\n",
    "    num_classes = len(class_names)\n",
    "    model = create_model(num_classes)\n",
    "    \n",
    "    # Display model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3)\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error training model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02bba6",
   "metadata": {},
   "source": [
    "### Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e478102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot the training and validation accuracy/loss\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Loss plot\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting training history: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a083737",
   "metadata": {},
   "source": [
    "## 4. Fine-tune the Model\n",
    "\n",
    "Let's fine-tune the model by unfreezing some layers of the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Unfreeze the top layers of the base model\n",
    "    base_model = model.layers[0]\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze all the layers except the last 30\n",
    "    fine_tune_at = len(base_model.layers) - 30\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Recompile the model with a lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Continue training with fine-tuning\n",
    "    fine_tune_history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Plot the fine-tuning history\n",
    "    plot_training_history(fine_tune_history)\n",
    "except Exception as e:\n",
    "    print(f\"Error during fine-tuning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7727c2",
   "metadata": {},
   "source": [
    "## 5. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55911f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('ksl_recognition_model.h5')\n",
    "\n",
    "# Save class names\n",
    "import json\n",
    "with open('class_names.json', 'w') as f:\n",
    "    json.dump(class_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2c6a0",
   "metadata": {},
   "source": [
    "## 6. Convert to TensorFlow Lite\n",
    "\n",
    "Now we'll convert our trained model to TensorFlow Lite format, which is optimized for mobile and edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de804dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model, quantize=True):\n",
    "    \"\"\"\n",
    "    Convert the trained model to TensorFlow Lite format\n",
    "    \"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if quantize:\n",
    "        # Apply quantization for size reduction\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # Apply full integer quantization\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "        \n",
    "        # Generate representative dataset from training data\n",
    "        def representative_dataset():\n",
    "            for i in range(10):\n",
    "                images, _ = next(iter(train_generator))\n",
    "                yield [images]\n",
    "        \n",
    "        converter.representative_dataset = representative_dataset\n",
    "    \n",
    "    # Convert the model\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    return tflite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert to TFLite without quantization\n",
    "    tflite_model = convert_to_tflite(model, quantize=False)\n",
    "    \n",
    "    # Save the TFLite model\n",
    "    with open('ksl_recognition_model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(\"TFLite model size:\", round(len(tflite_model) / (1024 * 1024), 2), \"MB\")\n",
    "    \n",
    "    # Try quantized version if the unquantized one works\n",
    "    try:\n",
    "        tflite_model_quant = convert_to_tflite(model, quantize=True)\n",
    "        with open('ksl_recognition_model_quantized.tflite', 'wb') as f:\n",
    "            f.write(tflite_model_quant)\n",
    "        print(\"Quantized TFLite model size:\", round(len(tflite_model_quant) / (1024 * 1024), 2), \"MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Quantization failed: {e}. Using non-quantized model.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error converting to TFLite: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4267d",
   "metadata": {},
   "source": [
    "## 7. Test the TFLite Model\n",
    "\n",
    "Let's verify our TFLite model works correctly by testing it on a few sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tflite_model(tflite_model_path, test_image_path, class_names):\n",
    "    \"\"\"\n",
    "    Test the TFLite model on a sample image\n",
    "    \"\"\"\n",
    "    # Load the TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Load and preprocess the test image\n",
    "    img = Image.open(test_image_path)\n",
    "    img = img.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = np.array(img, dtype=np.float32)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Set the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get the output results\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(output_data[0])\n",
    "    confidence = output_data[0][predicted_class]\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Prediction: {class_names[predicted_class]} (Confidence: {confidence:.2f})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return class_names[predicted_class], confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9becb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the TFLite model, uncomment and run this code with a test image path\n",
    "'''\n",
    "test_image_path = \"path/to/test/image.jpg\"  # Replace with actual test image path\n",
    "tflite_model_path = \"ksl_recognition_model.tflite\"\n",
    "\n",
    "# Load class names\n",
    "with open('class_names.json', 'r') as f:\n",
    "    class_names = json.load(f)\n",
    "    \n",
    "# Test the model\n",
    "predicted_class, confidence = test_tflite_model(tflite_model_path, test_image_path, class_names)\n",
    "print(f\"Predicted sign: {predicted_class} with {confidence:.2%} confidence\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a029731",
   "metadata": {},
   "source": [
    "## 8. Create a Simple Inference Script\n",
    "\n",
    "This script can be used to recognize Kenyan Sign Language in real-time using a webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a8c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_script():\n",
    "    \"\"\"\n",
    "    Create a Python script for real-time KSL recognition\n",
    "    \"\"\"\n",
    "    script = '''\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"ksl_recognition_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load class names\n",
    "with open(\"class_names.json\", \"r\") as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "# Open a video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Prepare the image for inference\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Set the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get the output results\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(output_data[0])\n",
    "    confidence = output_data[0][predicted_class]\n",
    "    \n",
    "    # Display the result on the frame\n",
    "    prediction_text = f\"{class_names[predicted_class]}: {confidence:.2f}\"\n",
    "    cv2.putText(frame, prediction_text, (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow('Kenyan Sign Language Recognition', frame)\n",
    "    \n",
    "    # Quit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''\n",
    "    \n",
    "    # Save the script\n",
    "    with open('ksl_recognition_app.py', 'w') as f:\n",
    "        f.write(script)\n",
    "    \n",
    "    print(\"Inference script saved as 'ksl_recognition_app.py'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inference script\n",
    "create_inference_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262aecde",
   "metadata": {},
   "source": [
    "## 9. Export the Model for Use in Mobile Applications\n",
    "\n",
    "We can also prepare our model for use in mobile applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file with the TFLite model and class names\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    output_dir = 'ksl_recognition_model'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy the files to the output directory\n",
    "    shutil.copy('ksl_recognition_model.tflite', f\"{output_dir}/\")\n",
    "    shutil.copy('class_names.json', f\"{output_dir}/\")\n",
    "    \n",
    "    # Create a README file\n",
    "    readme_content = \"\"\"\n",
    "# Kenyan Sign Language Recognition Model\n",
    "\n",
    "This package contains a TensorFlow Lite model for recognizing Kenyan Sign Language (KSL) and translating it to English.\n",
    "\n",
    "## Files:\n",
    "- ksl_recognition_model.tflite: The trained TFLite model\n",
    "- class_names.json: JSON file with the mapping of class indices to sign meanings\n",
    "\n",
    "## Model Information:\n",
    "- Input shape: 224x224x3 (RGB image normalized to [0,1])\n",
    "- Output: Class probabilities corresponding to different KSL signs\n",
    "\n",
    "## Usage:\n",
    "1. Load the model using TensorFlow Lite\n",
    "2. Preprocess the input image to 224x224 RGB\n",
    "3. Normalize the pixel values to [0,1]\n",
    "4. Run inference\n",
    "5. Get the class with the highest probability\n",
    "6. Map the class index to the sign meaning using class_names.json\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f\"{output_dir}/README.md\", 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    # Create a zip file\n",
    "    shutil.make_archive('ksl_recognition_model', 'zip', output_dir)\n",
    "    \n",
    "    print(\"Model package created: ksl_recognition_model.zip\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model package: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb7e50",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "We have successfully created a TensorFlow Lite model for Kenyan Sign Language recognition. The model can be used in various applications such as mobile apps, web applications, or embedded systems to help bridge the communication gap between Kenyan Sign Language users and English speakers.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Improve the model**: Collect more data, try different architectures, or use more advanced techniques like attention mechanisms.\n",
    "2. **Develop a complete mobile application**: Create Android/iOS apps that use the TFLite model for real-time sign language translation.\n",
    "3. **Add sentence construction**: Extend the model to recognize sequences of signs and construct meaningful English sentences.\n",
    "4. **Enable two-way translation**: Add functionality to translate English text back to Kenyan Sign Language animations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
